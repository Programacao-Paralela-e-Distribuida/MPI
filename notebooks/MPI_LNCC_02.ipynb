{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKma-XHN-b7e",
        "outputId": "14fc05fb-88e0-444b-972f-76c2abbb05db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MPI'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 146 (delta 10), reused 5 (delta 5), pack-reused 120 (from 1)\u001b[K\n",
            "Receiving objects: 100% (146/146), 3.94 MiB | 19.60 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Programacao-Paralela-e-Distribuida/MPI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv MPI/* .\n",
        "!rm -rf MPI\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS73IQ0yAcMA",
        "outputId": "edc77b37-a6c8-4092-acfa-026f6c81c17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot move 'MPI/bin' to './bin': Directory not empty\n",
            "mv: cannot move 'MPI/docs' to './docs': Directory not empty\n",
            "mv: cannot move 'MPI/src' to './src': Directory not empty\n",
            "bin  docs  Makefile  README.md\tsample_data  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Broadcast\n",
        "\n",
        "Um padrão de comunicação que envolva todos os processos em um comunicador é chamada de comunicação coletiva. Uma difusão (broadcast) é uma comunicação coletiva na qual um único processo envia os mesmos dados para cada processo.\n",
        "\n",
        "\n",
        "**int MPI_Bcast (void* mensagem, int cont, MPI_Datatype tipo_mpi, int raiz, MPI_Comm com)**\n",
        "\n",
        "\n",
        "A função **MPI_Bcast()** envia uma cópia dos dados de mensagem  no processo raiz para cada processo no mesmo comunicador. Para que a operação funcione corretamente, ela deve ser chamado por todos os processos no comunicador com os mesmos argumentos para *raiz* e *comunicador*.\n",
        "\n",
        "Uma mensagem de difusão não pode ser recebida com **MPI_Recv()**, pois a operação é exclusivamente coletiva. Os parâmetros cont e tipo_mpi  têm a mesma função que nas funções **MPI_Send()** e **MPI_Recv()**, definindo o tamanho e o tipo da mensagem.\n",
        "\n",
        "Contudo, ao contrário das funções ponto-a-ponto, o padrão MPI exige que os valores dps parâmetros do total e tipo dos elementos enviados sejam  iguais  para todos os processos dentro do mesmo comunicador em uma comunicação coletiva.\n",
        "\n",
        "No exemplo a seguir, o valor recebido pelo processo raiz é enviado para todos os processos no comunicador.\n",
        "\n"
      ],
      "metadata": {
        "id": "32n2biuX_fQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/mpi_bcast.c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsKvWXZvAZ0r",
        "outputId": "098a1cbe-2a48-4171-b3d5-c22b3a4cd659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "#include \"mpi.h\"\n",
            "\n",
            "int main(int argc, char *argv[]) { /* mpi_bcast.c  */\n",
            "int valor, meu_ranque, raiz = 0;\n",
            "\n",
            "    MPI_Init(&argc, &argv);\n",
            "    MPI_Comm_rank(MPI_COMM_WORLD, &meu_ranque);\n",
            "    /* Cada processo tem um valor inicial diferente */\n",
            "    valor = meu_ranque;\n",
            "    /* O valor a ser enviado é lido pelo processo raiz */\n",
            "    if (meu_ranque == raiz) {\n",
            "        printf(\"Entre um valor: \\n\");\n",
            "        scanf(\"%d\", &valor);    \n",
            "    }\n",
            "    /* A rotina de difusão é chamada por todos processos, apenas o processo raiz envia, os demais recebem */\n",
            "    MPI_Bcast(&valor, 1, MPI_INT, raiz, MPI_COMM_WORLD);\n",
            "    /* O valor agora é o mesmo em todos os processos */\n",
            "    printf(\"O processo com ranque %d recebeu o valor: %d\\n\",meu_ranque, valor);\n",
            "    /* Termina a execução */\n",
            "    MPI_Finalize();\n",
            "    return 0;\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc -o bin/mpi_bcast src/mpi_bcast.c\n",
        "!echo \"33\" | mpirun --allow-run-as-root -np 4 bin/mpi_bcast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdI54to5AqOb",
        "outputId": "56c4bab2-8207-455d-9c88-246e83689a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O processo com ranque 2 recebeu o valor: 33\n",
            "O processo com ranque 3 recebeu o valor: 33\n",
            "Entre um valor: \n",
            "O processo com ranque 0 recebeu o valor: 33\n",
            "O processo com ranque 1 recebeu o valor: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Coleta\n",
        "A operação de coleta é realizada pela função **MPI_Gather()**.\n",
        "\n",
        "**int MPI_Gather(void* vet_envia, int cont_envia, MPI_Datatype tipo_envia, void* vet_recebe, int cont_recebe, MPI_Datatype tipo_recebe, int raiz, MPI_comm com)**\n",
        "\n",
        "Cada processo no comunicador *com* envia o conteúdo de *vet_envi*a para o processo com ranque igual a *raiz*.  O processo raiz concatena os dados que são recebidos em *vet_recebe*  em uma ordem que é definida pelo ranque de cada processo.\n",
        "\n",
        "Os argumentos que terminam com *recebe* são significativos apenas no processo com ranque igual a raiz. O argumento *cont_recebe* indica o número de itens enviados por cada processo, não número total de itens recebidos pelo processo raiz e, normalmente, é igual ao argumento *cont_envia*.\n",
        "\n",
        "No exemplo a seguir, o processo raiz recebe um vetor com 10 elementos de cada processo (dele mesmo inclusive) e concatena em um vetor de *num_procs* $\\times$10  elementos e imprime o resultado."
      ],
      "metadata": {
        "id": "p1m3g28xB6rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc -o bin/mpi_gather src/mpi_gather.c\n",
        "!mpirun --allow-run-as-root -np 4 bin/mpi_gather"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JefRw9mGD8qt",
        "outputId": "ec3725b1-b7c1-41bb-bcb6-fb7fbffbda08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processo =  0, recebeu 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/mpi_gather.c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7ZJtjq-D1hC",
        "outputId": "c67c8b69-628a-4d09-d491-be6bdb1663c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "#include <stdlib.h>\n",
            "#include \"mpi.h\"\n",
            "#define TAM_VET 10\n",
            "\n",
            "int main(int argc, char *argv[]) { /* mpi_gather.c  */\n",
            "int meu_ranque, num_procs, i, raiz = 0;\n",
            "int *vet_recebe,  vet_envia[TAM_VET];\n",
            "\t\n",
            "\tMPI_Init(&argc, &argv);\n",
            "\tMPI_Comm_rank(MPI_COMM_WORLD, &meu_ranque);\n",
            "\tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
            "\t/* O processo raiz aloca o espaço de memória para o vetor de recepção */\n",
            "\tif (meu_ranque == raiz)\n",
            "        vet_recebe = (int *) malloc(num_procs*TAM_VET*sizeof(int)); \n",
            "    /* Cada processo atribui valor inicial ao vetor de envio */\n",
            "\tfor (i = 0; i < TAM_VET; i++) \n",
            "         vet_envia[i] = meu_ranque;\n",
            "    /* O vetor é recebido pelo processo raiz com as partes recebidas de todos processos, incluindo o processo raiz */\n",
            "\tMPI_Gather (vet_envia, TAM_VET, MPI_INT, vet_recebe, TAM_VET, MPI_INT, raiz, MPI_COMM_WORLD);\n",
            "\t/* O processo raiz imprime o vetor recebido */ \n",
            "\tif (meu_ranque == raiz) {\n",
            "\t    printf(\"Processo =  %d, recebeu\", meu_ranque);\n",
            "\t    for (i = 0; i < (TAM_VET*num_procs); i ++) {\n",
            "             printf(\" %d\", vet_recebe[i]);\n",
            "\t    }\n",
            "\t    printf(\"\\n\");\n",
            "\t}\n",
            "\t/* Termina a execução */\n",
            "\tMPI_Finalize();\n",
            "\treturn(0);\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Distribuição\n",
        "\n",
        "A operação de distribuição (scatter) é realizada pela função MPI_Scatter() do MPI.\n",
        "\n",
        "**int MPI_Scatter(void* vet_envia, int cont_envia, MPI_Datatype tipo_envia, void* vet_recebe, int cont_recebe, MPI_Datatype tipo_recebe, int raiz, MPI_Comm com)**\n",
        "\n",
        "O processo com o ranque igual a raiz distribui o conteúdo de *vet_envia* entre os *p* processos.  O conteúdo de *vet_envia* é dividido em *p* segmentos, cada um deles consistindo de *cont_envia* itens.\n",
        "O primeiro segmento vai para o processo com ranque 0, o segundo para  o processo com ranque  1, e assim por diante. Os argumentos que terminam com *envia* são significativos apenas no processo raiz, pois ele é o único que possui o conteúdo completo a ser distribuído.\n",
        "\n",
        "O exemplo a seguir o processo raiz monta um vetor com *num_procs* $\\times$10  elementos e envia um vetor de  10 elementos para cada processo, ele mesmo incluído.\n"
      ],
      "metadata": {
        "id": "13EeRO9jGr2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/mpi_scatter.c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7BDnp31JBC9",
        "outputId": "68b5b3f9-2dd6-4233-abf7-d8acf212d466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "#include <stdlib.h>\n",
            "#include \"mpi.h\"\n",
            "#define TAM_VET 10\n",
            "\n",
            "int main(int argc, char *argv[]) { /* mpi_scatter.c  */\n",
            "int i, meu_ranque, num_procs, raiz = 0;\n",
            "int *vet_envia, vet_recebe[TAM_VET];\n",
            "\t\n",
            "\tMPI_Init(&argc, &argv);\n",
            "    \tMPI_Comm_rank(MPI_COMM_WORLD, &meu_ranque);\n",
            "    \tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
            "    \t/* O processo raiz aloca o espaço de memória e inicia o vetor */\n",
            "    \tif (meu_ranque == raiz) {\n",
            "        \tvet_envia = (int*) malloc (num_procs*TAM_VET*sizeof(int));\n",
            "        \tfor (i = 0; i < (TAM_VET*num_procs); i++) \n",
            "             \t\tvet_envia[i] = i;\n",
            "    \t}\n",
            "    \t/* O vetor é distribuído em partes iguais entre os processos, incluindo o processo raiz */\n",
            "    \tMPI_Scatter(vet_envia, TAM_VET, MPI_INT, vet_recebe, TAM_VET, MPI_INT, raiz, MPI_COMM_WORLD);\n",
            "\t/* Cada processo imprime a parte que recebeu */ \n",
            "\tprintf(\"Processo =  %d, recebeu\", meu_ranque);\n",
            "\tfor (i = 0; i < TAM_VET; i ++) \n",
            "\t\tprintf(\" %d\", vet_recebe[i]);\n",
            "\tprintf(\"\\n\");\n",
            "\t/* Termina a execução */\n",
            "\tMPI_Finalize();\n",
            "\treturn(0);\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc -o bin/mpi_scatter src/mpi_scatter.c\n",
        "!mpirun --allow-run-as-root -np 4 bin/mpi_scatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjCtOCUmJFtW",
        "outputId": "d40308b0-7cec-421e-8ce8-9265f65ea84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processo =  0, recebeu 0 1 2 3 4 5 6 7 8 9\n",
            "Processo =  1, recebeu 10 11 12 13 14 15 16 17 18 19\n",
            "Processo =  2, recebeu 20 21 22 23 24 25 26 27 28 29\n",
            "Processo =  3, recebeu 30 31 32 33 34 35 36 37 38 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redução\n",
        "\n",
        "Em uma operação global de redução, todos os processos em um comunicador contribuem com dados que são combinados em operações binárias.\n",
        "Operações binárias típicas são a adição, máximo, mínimo, e lógico, etc.\n",
        "É possível definir operações adicionais além das mostradas para a função **MPI_Reduce()**.\n",
        "\n",
        "**int MPI_Reduce(void* operando, void* resultado, int cont, MPI_Datatype tipo_mpi, MPI_Op oper, int raiz, MPI_Comm com)**\n",
        "\n",
        "A operação MPI_Reduce() combina os operandos armazenados em *operando* usando a operação *oper* e armazena o resultado em *resultado*  no processo raiz. Tanto *operando* como *resultado* referem-se a *cont* posições de memória com o tipo *tipo_mpi*.\n",
        "\n",
        "**MPI_Reduce()** deve ser chamada por todos os processos no comunicador *com*, os valores de *cont*, *tipo_mpi* e *oper* devem ser os mesmos em  todos processos.\n",
        "\n",
        "O exemplo a seguir calcula o valor máximo para cada uma das 5 posições de um vetor."
      ],
      "metadata": {
        "id": "m9y4pMCmJVuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/mpi_reduce.c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2GZVYGiKqFW",
        "outputId": "6c8e9062-d0fd-4634-e1cf-7d64b42d6c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h> \n",
            "#include <math.h>\n",
            "#include \"mpi.h\" \n",
            "#define TAM 5\n",
            "\n",
            "int main(int argc, char *argv[]) { /* mpi_reduce.c  */\n",
            "int meu_ranque, num_procs, i, raiz = 0;      \n",
            "float vet_envia [TAM] ; /* Vetor a ser enviado  */ \n",
            "float vet_recebe [TAM]; /* Vetor a ser recebido */  \n",
            "\n",
            "    MPI_Init(&argc, &argv); \n",
            "    MPI_Comm_rank(MPI_COMM_WORLD, &meu_ranque); \n",
            "    MPI_Comm_size(MPI_COMM_WORLD, &num_procs); \n",
            "    /* Preenche o vetor com valores que dependem do ranque */\n",
            "    for (i = 0; i < TAM; i++)  \n",
            "        vet_envia[i] = (float) (meu_ranque*TAM+i);\n",
            "    /* Faz a redução, encontrando o valor máximo do vetor */\n",
            "    MPI_Reduce(vet_envia, vet_recebe, TAM, MPI_FLOAT, MPI_MAX, raiz, MPI_COMM_WORLD); \n",
            "    /* O processo raiz imprime o resultado da operação de redução */\n",
            "    if (meu_ranque == raiz) { \n",
            "        for (i = 0; i < TAM; i++) \n",
            "            printf(\"vet_recebe[%d] = %3.1f \", i,vet_recebe[i]); \n",
            "        printf(\"\\n\\n\");     } \n",
            "    MPI_Finalize();\n",
            "    return(0);\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc -o bin/mpi_reduce src/mpi_reduce.c\n",
        "!mpirun --allow-run-as-root -np 4 bin/mpi_reduce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHyxMZEcKvMT",
        "outputId": "da54117b-6f78-40c9-fd95-1a040f64dba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vet_recebe[0] = 15.0 vet_recebe[1] = 16.0 vet_recebe[2] = 17.0 vet_recebe[3] = 18.0 vet_recebe[4] = 19.0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2G3y9dZMtha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redução Global\n",
        "\n",
        "**int MPI_Allreduce (void* vet_envia, void* vet_recebe, int cont, MPI_Datatype tipo_mpi, MPI_Op oper, MPI_Comm com)**\n",
        "<br>\n",
        "<br>\n",
        "**MPI_Allreduce()** tem um resultado similar à operação **MPI_Reduce()**, com a diferença que o resultado da operação de redução *oper* é armazenado no vetor *vet_recebe* de cada processo envolvido na chamada e não apenas no processo raiz. Aliás, como todos recebem o resultado da redução, não há necessidade do parâmetro raiz no protótipo da função.\n",
        "<br>\n",
        "<br>\n",
        "Veja o exemplo a seguir, que calcula o valor aproximando de Pi, usando a aproximação pela fórmula:\n",
        "<br>\n",
        "<br>\n",
        "$\\pi = 4 \\int_0^1 \\frac{1}{1+x^2} \\, dx.$"
      ],
      "metadata": {
        "id": "_PhCv6i9LF6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/mpi_calcpi.c\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPNoihMNLP8v",
        "outputId": "12079d71-9f72-434e-d56f-6ec5507c6be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include<stdio.h>\n",
            "#include <math.h> \n",
            "#include \"mpi.h\"\n",
            "int main (int argc, char *argv[]){\t\n",
            "int meu_ranque, num_procs, n=10000000; \n",
            "double mypi, pi, h, x, sum = 0.0;\n",
            "\tMPI_Init(&argc, &argv);\n",
            "\tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
            "\tMPI_Comm_rank(MPI_COMM_WORLD, &meu_ranque);\n",
            "\th=1.0/(double) n;\n",
            "\tfor (int i = meu_ranque +1; i <= n; i += num_procs){\t\n",
            "\t\tx = h * ((double) i - 0.5);\n",
            "\t\tsum  += (4.0/(1.0 + x*x));\n",
            "\t}\n",
            "\tmypi = h* sum;\n",
            "\tMPI_Allreduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n",
            "\tprintf (\"valor aproximado de pi: %.16f \\n\", pi);\n",
            "\tMPI_Finalize( );\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}